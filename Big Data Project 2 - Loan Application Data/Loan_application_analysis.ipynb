{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models to Predict Loan Default\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Applicant Segmentation and Knowledge sharing with K-Mean](#part-3)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes into program\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# setting number of CPU cores to be used. * indicates all available cores\n",
    "master = \"local[*]\"\n",
    "# name of application\n",
    "app_name = \"Big Data App 2\"\n",
    "# setting spark config variable\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# SparkSession builder\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema_previous_application = schema = StructType([\n",
    "StructField(\"id_app\",IntegerType(),True),\n",
    "StructField(\"contract_type_previous\",IntegerType(),True),\n",
    "StructField(\"amt_annuity_previous\",FloatType(),True),\n",
    "StructField(\"amt_application\",FloatType(),True),\n",
    "StructField(\"amt_credit_previous\",FloatType(),True),\n",
    "StructField(\"amt_down_payment\",FloatType(),True),\n",
    "StructField(\"amt_goods_price_previous\",FloatType(),True),\n",
    "StructField(\"hour_appr_process_start\",IntegerType(),True),\n",
    "StructField(\"rate_down_payment\",FloatType(),True),\n",
    "StructField(\"rate_interest_primary\",FloatType(),True),\n",
    "StructField(\"rate_interest_privileged\",FloatType(),True),\n",
    "StructField(\"name_cash_loan_purpose\",StringType(),True),\n",
    "StructField(\"name_contract_status\",StringType(),True),\n",
    "StructField(\"days_decision\",IntegerType(),True),\n",
    "StructField(\"name_payment_type\",StringType(),True),\n",
    "StructField(\"code_rejection_reason\",StringType(),True),\n",
    "StructField(\"name_type_suite\",StringType(),True),\n",
    "StructField(\"name_client_type\",StringType(),True),\n",
    "StructField(\"name_goods_category\",StringType(),True),\n",
    "StructField(\"name_portfolio\",StringType(),True),\n",
    "StructField(\"name_product_type\",StringType(),True),\n",
    "StructField(\"channel_type\",StringType(),True),\n",
    "StructField(\"sellerplace_area\",IntegerType(),True),\n",
    "StructField(\"name_seller_industry\",StringType(),True),\n",
    "StructField(\"cnt_payment\",FloatType(),True),\n",
    "StructField(\"name_yield_group\",StringType(),True),\n",
    "StructField(\"product_combination\",StringType(),True),\n",
    "StructField(\"days_first_drawing\",FloatType(),True),\n",
    "StructField(\"days_first_due\",FloatType(),True),\n",
    "StructField(\"days_last_due_1st_version\",FloatType(),True),\n",
    "StructField(\"days_last_due\",FloatType(),True),\n",
    "StructField(\"days_termination\",FloatType(),True),\n",
    "StructField(\"nflag_insured_on_approval\",FloatType(),True),\n",
    "StructField(\"id\",LongType (),True)\n",
    "])\n",
    "\n",
    "schema_application = schema = StructType([ \n",
    "StructField(\"id_app\",IntegerType(),True),\n",
    "StructField(\"target\",IntegerType(),True),\n",
    "StructField(\"contract_type\",IntegerType(),True),\n",
    "StructField(\"gender\",StringType(),True),\n",
    "StructField(\"own_car\",StringType(),True),\n",
    "StructField(\"own_property\",StringType(),True),\n",
    "StructField(\"num_of_children\",IntegerType(),True),\n",
    "StructField(\"income_total\",FloatType(),True),\n",
    "StructField(\"amt_credit\",FloatType(),True),\n",
    "StructField(\"amt_annuity\",FloatType(),True),\n",
    "StructField(\"amt_goods_price\",FloatType(),True),\n",
    "StructField(\"income_type\",IntegerType(),True),\n",
    "StructField(\"education_type\",IntegerType(),True),\n",
    "StructField(\"family_status\",IntegerType(),True),\n",
    "StructField(\"housing_type\",IntegerType(),True),\n",
    "StructField(\"region_population\",FloatType(),True),\n",
    "StructField(\"days_birth\",IntegerType(),True),\n",
    "StructField(\"days_employed\",IntegerType(),True),\n",
    "StructField(\"own_car_age\",FloatType(),True),\n",
    "StructField(\"flag_mobile\",IntegerType(),True),\n",
    "StructField(\"flag_emp_phone\",IntegerType(),True),\n",
    "StructField(\"flag_work_phone\",IntegerType(),True),\n",
    "StructField(\"flag_cont_mobile\",IntegerType(),True),\n",
    "StructField(\"flag_phone\",IntegerType(),True),\n",
    "StructField(\"flag_email\",IntegerType(),True),\n",
    "StructField(\"occupation_type\",IntegerType(),True),\n",
    "StructField(\"cnt_fam_members\",FloatType(),True),\n",
    "StructField(\"weekday_app_process_start\",StringType(),True),\n",
    "StructField(\"hour_app_process_start\",IntegerType(),True),\n",
    "StructField(\"organization_type\",IntegerType(),True),\n",
    "StructField(\"credit_score_1\",FloatType(),True),\n",
    "StructField(\"credit_score_2\",FloatType(),True),\n",
    "StructField(\"credit_score_3\",FloatType(),True),\n",
    "StructField(\"days_last_phone_change\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_hour\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_day\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_week\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_month\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_quarter\",FloatType(),True),\n",
    "StructField(\"amt_credit_req_last_year\",FloatType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application = spark.read.format( \n",
    "    \"csv\").schema(schema_application).option( \n",
    "    \"header\", True).load(\"data/application_data.csv\") \n",
    "\n",
    "df_previous_application = spark.read.format( \n",
    "    \"csv\").schema(schema_previous_application).option( \n",
    "    \"header\", True).load(\"data/previous_application.csv\") \n",
    "\n",
    "df_value_dict = spark.read.format( \n",
    "    \"csv\").option( \n",
    "    \"header\", True).load(\"data/value_dict.csv\") \n",
    "\n",
    "df_loan_default = spark.read.format( \n",
    "    \"csv\").option( \n",
    "    \"header\", True).load(\"data/loan_default.csv\") \n",
    "\n",
    "df_application.printSchema()\n",
    "df_previous_application.printSchema()\n",
    "df_value_dict.printSchema()\n",
    "df_loan_default.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation and Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In this step, we’re going to perform data transformation and create some new features using existing information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding a new column loan_to_income_ratio which is the ratio between the loan amount and the income of the applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Adding new column loan_to_income_ratio\n",
    "df_application = df_application.withColumn(\"loan_to_income_ratio\", (col(\"amt_credit\")/col(\"income_total\")))\n",
    "# df_application.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.2.2. Performing age bucketing and create a new string column called age_bucket and set the values below:  \n",
    "    age < 25: Y  \n",
    "    25 <= age <35: E  \n",
    "    35 <= age <45: M  \n",
    "    45 <= age < 55: L  \n",
    "    55 <= age < 65: N  \n",
    "    Age >= 65: R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "\n",
    "# Creating a column with age values calculated from days_birth\n",
    "df_application = df_application.withColumn(\"age\", (F.abs(F.floor(\\\n",
    "                    F.col(\"days_birth\") / 365.2425))-1).\\\n",
    "                                        cast(IntegerType()))\n",
    "\n",
    "# Creating a column age_bucket using the values from age column\n",
    "df_application = df_application.withColumn(\"age_bucket\", when(col(\"age\") < 25, \"Y\")\n",
    "                                                        .when((col(\"age\") >= 25) & (col(\"age\") < 35), \"E\")\n",
    "                                                    .when((col(\"age\") >= 35) & (col(\"age\") < 45), \"M\")\n",
    "                                                    .when((col(\"age\") >= 45) & (col(\"age\") < 55), \"L\")\n",
    "                                                    .when((col(\"age\") >= 55) & (col(\"age\") < 65), \"N\")\n",
    "                                                    .when(col(\"age\") >= 65, \"R\")\n",
    "                                                    .otherwise(\"\"))\n",
    "\n",
    "# df_application.select(\"id_app\",\"age\",\"age_bucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a new string column named credit_worthiness. It takes the average value of credit_score_1,2,3. If the average >= 0.7, set credit_worthiness to “high”; 0.4 <= average <= 0.7 set to “medium”, < 0.4 set to “low”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting all null values to 0.5\n",
    "df_application = df_application.fillna({'credit_score_1':'0.5','credit_score_2':'0.5','credit_score_3':'0.5'})\n",
    "df_application = df_application.withColumn(\"avg_credit\", ((col(\"credit_score_1\") + col(\"credit_score_2\")\\\n",
    "                                                           + col(\"credit_score_3\")) / 3))\n",
    "df_application = df_application.withColumn(\"amt_to_income_ratio\", ((col(\"credit_score_1\") \\\n",
    "                                             + col(\"credit_score_2\") + col(\"credit_score_3\")) / 3))\n",
    "df_application = df_application.withColumn(\"credit_worthiness\", when(((col(\"credit_score_1\") \\\n",
    "                                            + col(\"credit_score_2\") + col(\"credit_score_3\")) / 3)>0.7, \"high\")\n",
    "                                    .when((((col(\"credit_score_1\") + col(\"credit_score_2\") + \\\n",
    "                                    col(\"credit_score_3\")) / 3)>=0.4) & (((col(\"credit_score_1\") + \\\n",
    "                                    col(\"credit_score_2\") + col(\"credit_score_3\")) / 3)<=0.7), \"medium\")\n",
    "                                    .when(((col(\"credit_score_1\") + col(\"credit_score_2\") + \\\n",
    "                                    col(\"credit_score_3\")) / 3)<0.4, \"low\")\n",
    "                                    .otherwise(\"\"))\n",
    "\n",
    "df_application.select(\"id_app\", \"credit_score_1\", \"credit_score_2\", \"credit_score_3\", \"avg_credit\",\\\n",
    "                      \"credit_worthiness\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating 4 columns: num_of_prev_app(number of previous applications), num_of_approved_app (number of approved applications), total_credit (sum of credit of all approved previous applications), total_credit_to_income_ratio (total credit/income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Joining Application Data and Previous Application \n",
    "joined_df = df_application.join(df_previous_application, \"id_app\")\n",
    "\n",
    "# Creating num_of_prev_app column\n",
    "prev_app_count = joined_df.groupBy(\"id_app\").count().withColumnRenamed(\"count\", \"num_of_prev_app\")\n",
    "joined_df = joined_df.join(prev_app_count, \"id_app\")\n",
    " \n",
    "# Creating num_of_approved_app column by filtering and grouping\n",
    "approved_applications = df_previous_application.filter(col(\"name_contract_status\") == \"Approved\")\n",
    "approved_applications_count = approved_applications.groupBy(\"id_app\").count()\\\n",
    ".withColumnRenamed(\"count\", \"num_of_approved_app\")\n",
    "joined_df = joined_df.join(approved_applications_count, \"id_app\")\n",
    "\n",
    "# Creating total_credit using approved applications\n",
    "total_credit = approved_applications.groupBy(\"id_app\").agg(F.sum(\"amt_credit_previous\").alias(\"total_credit\"))\n",
    "joined_df = joined_df.join(total_credit, \"id_app\").dropDuplicates([\"id_app\"])\n",
    "\n",
    "# Calculating total_credit_to_income_ratio using total_credit\n",
    "joined_df = joined_df.withColumn(\"total_credit_to_income_ratio\", (col(\"total_credit\")/col(\"income_total\")))\n",
    "\n",
    "# joined_df.printSchema()\n",
    "\n",
    "joined_df.select(\"id_app\", \"num_of_prev_app\", \"num_of_approved_app\",\"total_credit\", \\\n",
    "                 \"total_credit_to_income_ratio\").orderBy(\"id_app\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replacing education_type, occupation_type, income_type and family_status with matching strings from value_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_df = df_application.join(df_value_dict, (df_value_dict[\"category\"] == \\\n",
    "                                    \"education_type\") & ((df_application[\"education_type\"]) \\\n",
    "                                        == df_value_dict[\"value\"]), \"left_outer\") \\\n",
    "    .withColumn(\"mapped_value\", col(\"key\")) \\\n",
    "    .drop(\"id\", \"category\", \"key\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Join the loan_default data frame and add is_default to application data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_application_df = joined_df.join(df_loan_default, \"id_app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Printing 10 records from the application_data data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_application_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    " With the transformed data frame from 1.2, we write code to show the basic statistics:\n",
    "- For each numeric column, we show count, mean, stddev, min, max, 25 percentile, 50 percentile, and 75 percentile;  \n",
    "- For each non-numeric column, we display the top 5 based on counts in descending order;  \n",
    "- For each boolean column, we display the value and count(i.e., two rows in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns1 = [col(column) for column in final_application_df.columns if \\\n",
    "                    final_application_df.schema[column].dataType in [IntegerType()]]\n",
    "numeric_columns2 = [col(column) for column in final_application_df.columns if \\\n",
    "                    final_application_df.schema[column].dataType in [DoubleType()]]\n",
    "numeric_columns3 = [col(column) for column in final_application_df.columns if \\\n",
    "                    final_application_df.schema[column].dataType in [FloatType()]]\n",
    "non_numeric_columns = [col(column) for column in final_application_df.columns \\\n",
    "                       if final_application_df.schema[column].dataType not in \\\n",
    "                       [IntegerType(), FloatType(), DoubleType()]]\n",
    "boolean_cols = ['is_default','own_car','own_property','flag_emp_phone',\\\n",
    "                'flag_work_phone','flag_phone','flag_email']\n",
    "\n",
    "numeric_columns_stats = final_application_df.select(numeric_columns1).describe()\n",
    "numeric_columns_stats.show()\n",
    "\n",
    "numeric_columns_stats = final_application_df.select(numeric_columns2).describe()\n",
    "numeric_columns_stats.show()\n",
    "\n",
    "numeric_columns_stats = final_application_df.select(numeric_columns3).describe()\n",
    "numeric_columns_stats.show()\n",
    "\n",
    "for column in non_numeric_columns:\n",
    "    top_values = final_application_df.groupBy(column).count().orderBy(col(\"count\").desc()).limit(5)\n",
    "    print(f\"Top 5 values for {column} based on counts:\")\n",
    "    top_values.show(truncate=False)\n",
    "    \n",
    "# Display value and count for each boolean column\n",
    "for col_name in boolean_cols:\n",
    "    value_counts = final_application_df.groupBy(col_name).count()\n",
    "    print(f\"Counts for boolean column '{col_name}':\")\n",
    "    value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploring the data frame and writing code to present two plots worthy of presentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "df = final_application_df.toPandas().sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "# Choose the attribute you want to analyze (replace 'selected_attribute' with your attribute column)\n",
    "selected_attribute = 'total_credit_to_income_ratio'\n",
    "\n",
    "# Create a bar plot or box plot to visualize the relationship with the boolean column\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar Plot (for categorical variables)\n",
    "sns.boxplot(y=selected_attribute, x='is_default', data=df)\n",
    "\n",
    "# Box Plot (for numerical variables)\n",
    "# sns.boxplot(x='is_default', y=selected_attribute, data=df)\n",
    "\n",
    "plt.title(f'Correlation between {selected_attribute} and is_default')\n",
    "plt.ylabel(selected_attribute)\n",
    "plt.xlabel('is_default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, we try to correlate the attribute total_credit_to_income_ratio with is_default. From the graph we can see that higher the total_credit_to_income_ratio, more likely is the applicant to default. Logically this makes sense as well. a high total_credit_to_income_ratio indicates that the applicant is requesting for a higher loan credit than their income, i.e. higher the total_credit_to_income_ratio, greater is the payment period, hence higher the chances of defaulting. Majority of defaults occur when applicants apply for loans beyond their means, which is indicated by a high total_credit_to_income_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with your actual file)\n",
    "df = final_application_df.toPandas().sample(frac=0.1, replace=True, random_state=1)\n",
    "\n",
    "# Choose the attribute you want to analyze (replace 'selected_attribute' with your attribute column)\n",
    "selected_attribute = 'credit_worthiness'\n",
    "\n",
    "# Create a bar plot or box plot to visualize the relationship with the boolean column\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar Plot (for categorical variables)\n",
    "# sns.barplot(x=selected_attribute, y='is_default', data=df, ci=None)\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"credit_worthiness\", y=\"avg_credit\", hue=\"is_default\",\n",
    "    errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n",
    ")\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"credit_worthiness\", \"avg_credit\")\n",
    "g.legend.set_title(\"\")\n",
    "\n",
    "# Box Plot (for numerical variables)\n",
    "# sns.boxplot(x='is_default', y=selected_attribute, data=df)\n",
    "\n",
    "# plt.title(f'Correlation between {selected_attribute} and is_default')\n",
    "# plt.ylabel(selected_attribute)\n",
    "# plt.xlabel('is_default')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Above graph plots the total number of defaults for each category of credit_worthiness. There are 3 categories of credit_worthiness: high, medium and low. As is expected, there are more loan applicants from high credit worthiness than low or medium. The graph also indcates that for high credit_worthiness, the defaults are less than 50% of the high applicants. Whereas for low and medium, the defaults are greater than 50% of the low and medium applicants. This shows that credit scores are a worthy indicator for an applicants financial situation and their liability for defaults. Despite averaging the credit scores and grouping them into 3 categories, it is distinguishable to acertain the expectation of an applicants default. Credit scores are strong indicators of financial nous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We print the correlation matrix to find the columns that would be more or less useful for Feature extraction and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# plot_df = final_application_df.select(\"credit_worthiness\",\"total_credit_to_income_ratio\",\\\n",
    "#                                                    \"income_type\",\"housing_type\", \"is_default\")\n",
    "\n",
    "# final_application_df.show()\n",
    "\n",
    "categorical_columns = [\"sellerplace_area\",\"name_seller_industry\",\"cnt_payment\",\\\n",
    "                    \"name_yield_group\",\"product_combination\",\"days_first_drawing\",\\\n",
    "                    \"days_first_due\",\"days_last_due_1st_version\",\"days_last_due\",\\\n",
    "                       \"days_termination\",\"nflag_insured_on_approval\",\"id\",\"num_of_prev_app\",\\\n",
    "                       \"num_of_approved_app\",\"total_credit\",\"total_credit_to_income_ratio\",\"is_default\"]\n",
    "indexers = StringIndexer(inputCols=[column for column in categorical_columns], \\\n",
    "                         outputCols=[f\"{column}_index\" for column in categorical_columns])\n",
    "indexers.setHandleInvalid(\"keep\")\n",
    "df_transformed = indexers.fit(final_application_df).transform(final_application_df)\n",
    "\n",
    "# pipeline = Pipeline(stages=indexers)\n",
    "# df_transformed = pipeline.fit(final_application_df).transform(final_application_df)\n",
    "\n",
    "columnss = [f\"{column}_index\" for column in categorical_columns]\n",
    "# column_str = columnss.join(',')\n",
    "check_df = df_transformed.select(\"sellerplace_area_index\",\"name_seller_industry_index\",\\\n",
    "                                 \"cnt_payment_index\",\"name_yield_group_index\",\\\n",
    "                                 \"product_combination_index\",\"days_first_drawing_index\",\\\n",
    "                                 \"days_first_due_index\",\"days_last_due_1st_version_index\",\\\n",
    "                                 \"days_last_due_index\",\"days_termination_index\",\\\n",
    "                                 \"nflag_insured_on_approval_index\",\"id_index\",\\\n",
    "                                 \"num_of_prev_app_index\",\"num_of_approved_app_index\",\\\n",
    "                                 \"total_credit_index\",\"total_credit_to_income_ratio_index\",\"is_default_index\")\n",
    "\n",
    "train_data, test_data = check_df.randomSplit([0.99, 0.01], seed=42)\n",
    "\n",
    "plot1_pandas = test_data.toPandas()\n",
    "\n",
    "# print(plot1_pandas)\n",
    "\n",
    "correlation_matrix = plot1_pandas.corr()\n",
    "\n",
    "# Extract correlations between other columns and the target column\n",
    "target_correlations = correlation_matrix['is_default_index']\n",
    "\n",
    "# Print or analyze the correlation values\n",
    "print(target_correlations)\n",
    "\n",
    "# sns.scatterplot(x='total_credit_to_income_ratio_index', y='income_type', \\\n",
    "# hue='is_default', data=plot1_pandas, palette={'true': 'blue', 'false': 'red'})\n",
    "# plt.title(\"Scatter Plot with Color based on Boolean Value\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(plot1_pandas[\"total_credit_to_income_ratio_index\"], plot1_pandas\\\n",
    "# [\"credit_worthiness_index\"], c=plot1_pandas[\"is_default\"], cmap='Spectral')\n",
    "# plt.colorbar()\n",
    "# plt.title('Simple Scatter plot')\n",
    "# plt.xlabel('X - value')\n",
    "# plt.ylabel('Y - value')\n",
    "# plt.show()\n",
    "\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "# fig.suptitle('Sharing x per column, y per row')\n",
    "# ax1.plot(plot1_pandas[\"income_type_index\"], plot1_pandas[\"housing_type_index\"])\n",
    "# ax2.plot(plot1_pandas[\"housing_type_index\"], plot1_pandas[\"is_default\"], 'tab:orange')\n",
    "# ax3.plot(plot1_pandas[\"total_credit_to_income_ratio_index\"], plot1_pandas[\"is_default\"], 'tab:green')\n",
    "# ax4.plot(plot1_pandas[\"credit_worthiness_index\"], plot1_pandas[\"is_default\"], 'tab:red')\n",
    "# # plot1_df = chosen_ones.toPandas()\n",
    "# corr_matrix = plot1_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "\n",
    "### 2.1 Feature selection and preparation of the feature columns\n",
    "\n",
    "- Based on the data exploration from 1.2 and considering the use case, we discuss the importance of those features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate_interest_primary, rate_interest_privileged, occupation_type and cnt_fam_members can be removed\n",
    "as they have got a correlation index with is_default either >0.01 or <-0.01. That shows extremely low dependence on those attributes.\n",
    "additionally num_of_children, housing_type, days_birth, days_employed, age, channel_type \n",
    "along with the respective non index columns can also be removed as they have a correlation index barely over 0.01\n",
    "We can also group multiple columns together to increase their correlation index with is_default. In this example i will be\n",
    "grouping together credit_worthiness, total_credit_to_income_ratio, income_total, amt_credit, previous application data and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code to create/transform the columns based on your discussion above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_application_df.select(\"gender\",\"income_type\",\"education_type\",\"family_status\",\\\n",
    "                                 \"housing_type\",\"occupation_type\",\"loan_to_income_ratio\",\\\n",
    "                                 \"age_bucket\",\"credit_worthiness\",\"avg_credit\",\\\n",
    "                                 \"num_of_approved_app\",\"total_credit_to_income_ratio\",\"is_default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "- Code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1, and creating ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "categorical_columns=[\"gender\", \"education_type\", \"income_type\",\"family_status\",\\\n",
    "                     \"housing_type\",\"occupation_type\",\\\n",
    "                     \"is_default\"]\n",
    "categorical_columns_no_default=[\"gender\", \"education_type\", \"income_type\",\\\n",
    "                                \"family_status\",\"housing_type\",\"occupation_type\",\\\n",
    "                                ]\n",
    "\n",
    "# Feature Transformation\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\") for col in categorical_columns]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols= [f\"{col}_index\" for col in categorical_columns_no_default],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Model Estimators\n",
    "rf = RandomForestClassifier(labelCol=\"is_default_index\", featuresCol=\"features\", numTrees=10)\n",
    "gbt = GBTClassifier(labelCol=\"is_default_index\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code to include the above Transformers/Estimators into two pipelines(RF and GBT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=indexers + [assembler, rf])\n",
    "pipeline_gbt = Pipeline(stages=indexers + [assembler, gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare, Train and Evaluate models  \n",
    "- Code to split the data for training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code to use the corresponding ML Pipelines to train the models on the training data. Then using the trained models we predict the testing data from 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = pipeline_rf.fit(train_data)\n",
    "model_gbt  = pipeline_gbt.fit(train_data)\n",
    "\n",
    "predictions_rf = model_rf.transform(test_data)\n",
    "predictions_gbt = model_gbt.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For both models(RF and GBT) and testing data, we write code to display the count of TP/TN/FP/FN. Compute the AUC, accuracy, recall, and precision for the above-threshold/below-threshold label from each model testing result using pyspark MLlib/ML APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#For Random Forest\n",
    "# TP,TN,FP,FN Values\n",
    "\n",
    "tn_rf = predictions_rf.filter('prediction = 0.0 AND is_default_index = 0.0').count()\n",
    "tp_rf = predictions_rf.filter('prediction = 1.0 AND is_default_index = 1.0').count()\n",
    "fp_rf = predictions_rf.filter('prediction = 0.0 AND is_default_index = 1.0').count()\n",
    "fn_rf = predictions_rf.filter('prediction = 1.0 AND is_default_index = 0.0').count()\n",
    "\n",
    "predictions_rf.groupBy('is_default_index', 'prediction').count().show()\n",
    "\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy_rf = (tn_rf+tp_rf)/(tn_rf+tp_rf+fn_rf+fp_rf)\n",
    "precision_rf = tp_rf/(tp_rf+fp_rf)\n",
    "recall_rf = tp_rf/(tp_rf+fn_rf) \n",
    "\n",
    "auc_evaluator_rf = BinaryClassificationEvaluator(labelCol='is_default_index', metricName='areaUnderROC')\n",
    "auc_rf = auc_evaluator_rf.evaluate(predictions_rf)\n",
    "\n",
    "# roc_values = []\n",
    "# for threshold in np.linspace(0, 1, 100):        \n",
    "#         prob_df=prob_df.withColumn('prediction',F.when(prob_df.positive_prob > threshold,1).otherwise(0))\n",
    "#         tp,tn,fp,fn = confusion_matrix(prob_df)  \n",
    "#         tpr = tp/(tp+fn)\n",
    "#         fpr = fp/(fp+tn)\n",
    "#         print('Threshold:',threshold,'TPR:',tpr,'FPR:',fpr)\n",
    "#         roc_values.append([tpr, fpr])\n",
    "# tpr_values, fpr_values = zip(*roc_values)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,7))\n",
    "# ax.plot(fpr_values,tpr_values)\n",
    "# ax.plot(np.linspace(0, 1, 100),\n",
    "#          np.linspace(0, 1, 100),\n",
    "#          label='baseline',\n",
    "#          linestyle='--')\n",
    "# plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "# plt.ylabel('TPR', fontsize=16)\n",
    "# plt.xlabel('FPR', fontsize=16)\n",
    "# plt.legend(fontsize=12);\n",
    "\n",
    "print(\"For Random Forest\")\n",
    "print(f\"TP: {tp_rf}\")\n",
    "print(f\"TN: {tn_rf}\")\n",
    "print(f\"FP: {fp_rf}\")\n",
    "print(f\"FN: {fn_rf}\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"Precision: {precision_rf}\")\n",
    "print(f\"Recall: {recall_rf}\")\n",
    "print(f\"Area Under ROC Curve: {auc_rf}\")\n",
    "\n",
    "#For Gradient Boosted Trees\n",
    "# TP,TN,FP,FN Values\n",
    "\n",
    "tn_gbt = predictions_gbt.filter('prediction = 0.0 AND is_default_index = 0.0').count()\n",
    "tp_gbt = predictions_gbt.filter('prediction = 1.0 AND is_default_index = 1.0').count()\n",
    "fp_gbt = predictions_gbt.filter('prediction = 0.0 AND is_default_index = 1.0').count()\n",
    "fn_gbt = predictions_gbt.filter('prediction = 1.0 AND is_default_index = 0.0').count()\n",
    "\n",
    "predictions_gbt.groupBy('is_default_index', 'prediction').count().show()\n",
    "\n",
    "# calculate metrics by the confusion matrix\n",
    "accuracy_gbt = (tn_gbt+tp_gbt)/(tn_gbt+tp_gbt+fn_gbt+fp_gbt)\n",
    "precision_gbt = tp_gbt/(tp_gbt+fp_gbt)\n",
    "recall_gbt = tp_gbt/(tp_gbt+fn_gbt) \n",
    "\n",
    "auc_evaluator_gbt = BinaryClassificationEvaluator(labelCol='is_default_index', metricName='areaUnderROC')\n",
    "auc_gbt = auc_evaluator_gbt.evaluate(predictions_gbt)\n",
    "\n",
    "\n",
    "print(\"For Gradient Boosted Trees\")\n",
    "print(f\"TP: {tp_gbt}\")\n",
    "print(f\"TN: {tn_gbt}\")\n",
    "print(f\"FP: {fp_gbt}\")\n",
    "print(f\"FN: {fn_gbt}\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy_gbt}\")\n",
    "print(f\"Precision: {precision_gbt}\")\n",
    "print(f\"Recall: {recall_gbt}\")\n",
    "print(f\"Area Under ROC Curve: {auc_gbt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing the results of both models, I have decided to save Gradient Boosted Trees Model as it provides a higher accuracy, precision and greater Area under the ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbt.save('model/gbt_model2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Applicant Segmentation and Knowledge sharing with K-Mean <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "- Utilize K-Mean clustering/hyperparameter tuning to find the optimal K value and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Extracting required data\n",
    "df_kmeans = final_application_df.select(\"income_type\",\"avg_credit\",\\\n",
    "                                        \"total_credit_to_income_ratio\",\"own_car\",\\\n",
    "                                        \"own_property\",\"income_total\",\"housing_type\",\\\n",
    "                                        \"occupation_type\")\n",
    "\n",
    "indexer = StringIndexer(inputCols=[\"income_type\",\"own_car\",\"own_property\",\"housing_type\",\\\n",
    "                                   \"occupation_type\"],outputCols=[\"income_type_index\",\\\n",
    "                                    \"own_car_index\",\"own_property_index\",\"housing_type_index\",\\\n",
    "                                        \"occupation_type_index\"])\n",
    "indexed_transformer = indexer.fit(df_kmeans)\n",
    "indexed_data = indexed_transformer.transform(df_kmeans)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"income_type_index\",\"own_car_index\",\"own_property_index\",\\\n",
    "                                       \"housing_type_index\",\"occupation_type_index\",\"avg_credit\",\\\n",
    "                                       \"total_credit_to_income_ratio\",\"income_total\"],outputCol='features')\n",
    "assembled_data = assembler.transform(indexed_data)\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette_arr=[]\n",
    "for k in range(2,10):\n",
    "    k_means= KMeans(featuresCol='features', k=k)\n",
    "    model = k_means.fit(assembled_data)\n",
    "    predictions = model.transform(assembled_data)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_arr.append(silhouette)\n",
    "    print('No of clusters:',k,'Silhouette Score:',silhouette)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,10),silhouette_arr)\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this exercise, I have decided to segment the applicants based on their finances. \n",
    "As figuring out a persons finances arent as straight forward, I have decided to use few data points\n",
    "to determine an applicants finance. Income_type, income_total and occupation_type will help us figure out what \n",
    "the primary source of income is for each applicant. avg_credit gives the applicants credit score, which useful \n",
    "in helping identify their financial skills. total_credit_to_income_ratio tells us if the persons credit expenses \n",
    "are in line with their income. own_car and own_property are great identifiers of their financial situation, it could \n",
    "either tell us they can afford luxuries or they are in debt, this can be further strenghtened by housing_type.\n",
    "We can group all of these data points the segment the applicants. Using K means, I have Identified that the ideal \n",
    "number of clusters are 5. This has a Silhouette score of 0.7544790092012427 which strikes the right balance of \n",
    "segmentation and silhouette scores. Using this we can segment the applicants into 5 categories i.e. High, Mid-high, Mid\n",
    "Mid-low and Low finances. Naturally, MoLoCo can use this segmentation as a pre approvl process for loan amounts.\n",
    "This helps hedge against the risk of lending high amounts to applicants who may never be able to pay of the loans.\n",
    "Based on combinations of the above 8 factors, applicants can be grouped into the 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
